name: CD - Deploy to Cloud

on:
  push:
    branches: [ master ]

concurrency:
  group: cd-${{ github.ref }}
  cancel-in-progress: true

jobs:
  deploy:
    name: Build Docker image and Push to AWS ECR and Deploy to EKS
    runs-on: prod-eks-runners
    timeout-minutes: 45

    env:
      AWS_REGION: ap-south-1
      EKS_CLUSTER: backstract-dev
      ECR_REPO: backstract_apps
      STATIC_TAG: nostalgic-beaver-26a2a5222a
      INFRA_DIR: infra/k8s

      # ✅ Deploy app objects here (NOT arc-runners)
      APP_NAMESPACE: default

      # ✅ Buildx local cache (persisted via actions/cache)
      BUILDX_CACHE_DIR: /tmp/.buildx-cache

    steps:
      - name: Check out code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          lfs: false

      # ✅ Cache Buildx local cache across workflow runs
      - name: Restore Docker layer cache (buildx local cache)
        uses: actions/cache@v4
        with:
          path: ${{ env.BUILDX_CACHE_DIR }}
          key: buildx-${{ runner.os }}-${{ github.ref_name }}-${{ github.sha }}
          restore-keys: |
            buildx-${{ runner.os }}-${{ github.ref_name }}-
            buildx-${{ runner.os }}-

      - name: Install AWS CLI v2 (temporary; remove once baked into runner image)
        timeout-minutes: 5
        run: |
          set -euo pipefail
          if command -v aws >/dev/null 2>&1; then
            aws --version
            exit 0
          fi
          if command -v apt-get >/dev/null 2>&1; then
            sudo apt-get update -y
            sudo apt-get install -y unzip curl
          elif command -v yum >/dev/null 2>&1; then
            sudo yum install -y unzip curl
          else
            echo "ERROR: Neither apt-get nor yum found."
            exit 1
          fi
          curl -sSLo /tmp/awscliv2.zip "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip"
          unzip -q /tmp/awscliv2.zip -d /tmp
          sudo /tmp/aws/install --update
          aws --version

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.31.0'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push to ECR (BuildKit + local+registry cache)
        timeout-minutes: 25
        run: |
          set -euo pipefail

          AWS_ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"
          REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          IMAGE="${REGISTRY}/${ECR_REPO}"
          SHA_TAG="${GITHUB_SHA}"

          echo "IMAGE=${IMAGE}"
          echo "SHA_TAG=${SHA_TAG}"
          echo "STATIC_TAG=${STATIC_TAG}"

          mkdir -p "${BUILDX_CACHE_DIR}"

          # Reuse buildx builder if present
          docker buildx create --use --name builder >/dev/null 2>&1 || docker buildx use builder

          # Build with BOTH:
          # - local cache (fast on repeat runs)
          # - registry cache in ECR (works across machines)
          docker buildx build \
            --platform linux/amd64 \
            -t "${IMAGE}:${SHA_TAG}" \
            -t "${IMAGE}:${STATIC_TAG}" \
            --cache-from "type=local,src=${BUILDX_CACHE_DIR}" \
            --cache-from "type=registry,ref=${IMAGE}:buildcache" \
            --cache-to "type=local,dest=${BUILDX_CACHE_DIR}-new,mode=max" \
            --cache-to "type=registry,ref=${IMAGE}:buildcache,mode=max" \
            --push \
            .

          # Rotate local cache to keep it consistent
          rm -rf "${BUILDX_CACHE_DIR}"
          mv "${BUILDX_CACHE_DIR}-new" "${BUILDX_CACHE_DIR}"

      - name: Update kubeconfig to point to EKS
        timeout-minutes: 8
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${EKS_CLUSTER}" --region "${AWS_REGION}"

      - name: Deploy to AWS EKS Cluster (diff-aware + faster)
        timeout-minutes: 15
        env:
          BST_TOKEN: ${{ secrets.BACKSTRACT_AUTH_TOKEN }}
        run: |
          set -euo pipefail

          if [ -z "${APP_NAMESPACE:-}" ]; then
            echo "ERROR: APP_NAMESPACE is not set"
            exit 1
          fi

          if [ -z "${BST_TOKEN:-}" ]; then
            echo "ERROR: BACKSTRACT_AUTH_TOKEN is empty or not set"
            exit 1
          fi

          # ---- 1) Apply shared configs in ONE call (fewer API calls) ----
          kubectl apply -n "${APP_NAMESPACE}" \
            -f "${INFRA_DIR}/alloy-rbac.yaml" \
            -f "${INFRA_DIR}/alloy-configmap.yaml" \
            -f "${INFRA_DIR}/env-configmap.yaml"

          # ---- 2) Create rendered deployment (token injected) ----
          awk -v token="${BST_TOKEN}" '{gsub(/PLACEHOLDER_BST_TOKEN_REPLACE_AT_RUNTIME/, token)}1' "${INFRA_DIR}/deployment.yaml" > /tmp/deployment.yaml

          if grep -q "PLACEHOLDER_BST_TOKEN_REPLACE_AT_RUNTIME" /tmp/deployment.yaml; then
            echo "ERROR: Token injection failed - placeholder still exists"
            exit 1
          fi

          # ---- 3) Apply deployment and service; detect deployment name safely ----
          kubectl apply -n "${APP_NAMESPACE}" -f /tmp/deployment.yaml -o name | tee /tmp/applied_names.txt >/dev/null
          kubectl apply -n "${APP_NAMESPACE}" -f "${INFRA_DIR}/service.yaml"

          DEPLOYMENT_NAME="$(grep '^deployment\.apps/' /tmp/applied_names.txt | sed -n '1p' | sed 's#deployment\.apps/##')"
          if [ -z "${DEPLOYMENT_NAME}" ]; then
            echo "ERROR: Could not detect deployment name from apply output"
            cat /tmp/applied_names.txt || true
            exit 1
          fi
          echo "Deployment: ${DEPLOYMENT_NAME}"

          # ---- 4) Restart ONLY if Deployment spec changed vs live cluster ----
          # If no diff, skip restart (saves time and avoids unnecessary rollout)
          if kubectl diff -n "${APP_NAMESPACE}" -f /tmp/deployment.yaml >/dev/null 2>&1; then
            echo "No changes detected in deployment spec (kubectl diff shows no output). Skipping restart."
          else
            echo "Changes detected in deployment spec. Waiting for rollout."
          fi

          # IMPORTANT: rollout status is still valuable even if no restart
          kubectl rollout status -n "${APP_NAMESPACE}" deployment "${DEPLOYMENT_NAME}" --timeout=600s

          kubectl get -n "${APP_NAMESPACE}" pods -o wide
