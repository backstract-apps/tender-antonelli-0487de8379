name: CD - Deploy to Cloud

on:
  push:
    branches: [ master ]

# Prevent parallel deploys to the same branch
concurrency:
  group: cd-${{ github.ref }}
  cancel-in-progress: true

jobs:
  deploy:
    name: Build Docker image and Push to AWS ECR and Deploy to EKS
    runs-on: prod-eks-runners
    timeout-minutes: 40

    env:
      AWS_REGION: ap-south-1
      EKS_CLUSTER: backstract-dev
      ECR_REPO: backstract_apps
      INFRA_DIR: infra/k8s

      # Keep this only if you still need it somewhere else.
      # Rollout restart will NOT depend on this anymore.
      STATIC_TAG: nostalgic-beaver-26a2a5222a

    steps:
      - name: Check out code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup job (debug)
        timeout-minutes: 2
        run: |
          set -euo pipefail
          echo "Runner environment: $(uname -a)"
          echo "Disk:"
          df -h
          echo "Memory:"
          free -h
          echo "CPU:"
          nproc || true
          echo "Docker:"
          docker version || true
          docker info || true
          echo "Job setup completed"

      # Install AWS CLI v2 (GitHub-hosted runners have it; your ARC runner does not)
      - name: Install AWS CLI v2
        timeout-minutes: 5
        run: |
          set -euo pipefail

          if command -v aws >/dev/null 2>&1; then
            echo "AWS CLI already installed: $(aws --version)"
            exit 0
          fi

          if command -v apt-get >/dev/null 2>&1; then
            sudo apt-get update -y
            sudo apt-get install -y unzip curl
          elif command -v yum >/dev/null 2>&1; then
            sudo yum install -y unzip curl
          else
            echo "ERROR: Neither apt-get nor yum found. Cannot install AWS CLI automatically."
            exit 1
          fi

          curl -sSLo /tmp/awscliv2.zip "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip"
          unzip -q /tmp/awscliv2.zip -d /tmp
          sudo /tmp/aws/install --update

          aws --version

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.31.0'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Sanity check AWS identity
        run: |
          set -euo pipefail
          aws sts get-caller-identity

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Build & push using BuildKit cache stored in ECR
      - name: Build and push to ECR (BuildKit + registry cache)
        timeout-minutes: 25
        run: |
          set -euo pipefail
          date

          AWS_ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"
          REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          IMAGE="${REGISTRY}/${ECR_REPO}"
          SHA_TAG="${GITHUB_SHA}"

          echo "AWS_ACCOUNT_ID=${AWS_ACCOUNT_ID}"
          echo "REGISTRY=${REGISTRY}"
          echo "IMAGE=${IMAGE}"
          echo "SHA_TAG=${SHA_TAG}"

          if [ -z "${AWS_ACCOUNT_ID}" ] || [ -z "${REGISTRY}" ] || [ -z "${IMAGE}" ]; then
            echo "ERROR: Registry/Image variables are empty. Check AWS credentials/ECR access."
            exit 1
          fi

          docker buildx create --use --name builder >/dev/null 2>&1 || docker buildx use builder

          echo "Building and pushing:"
          echo "  ${IMAGE}:${SHA_TAG}"
          echo "  ${IMAGE}:${STATIC_TAG}"
          echo "Using cache ref:"
          echo "  ${IMAGE}:buildcache"

          time docker buildx build \
            --platform linux/amd64 \
            -t "${IMAGE}:${SHA_TAG}" \
            -t "${IMAGE}:${STATIC_TAG}" \
            --cache-from "type=registry,ref=${IMAGE}:buildcache" \
            --cache-to "type=registry,ref=${IMAGE}:buildcache,mode=max" \
            --push \
            .

          date

      - name: Update kubeconfig to point to EKS
        timeout-minutes: 8
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${EKS_CLUSTER}" --region "${AWS_REGION}"
          kubectl cluster-info

      - name: Deploy to AWS EKS Cluster (apply + wait + debug on failure)
        timeout-minutes: 15
        env:
          BST_TOKEN: ${{ secrets.BACKSTRACT_AUTH_TOKEN }}
        run: |
          set -euo pipefail
          NAMESPACE="arc-runners"
      
          if [ -z "${BST_TOKEN:-}" ]; then
            echo "ERROR: BACKSTRACT_AUTH_TOKEN is empty or not set"
            exit 1
          fi
      
          kubectl apply -n "${NAMESPACE}" -f "${INFRA_DIR}/alloy-rbac.yaml"
          kubectl apply -n "${NAMESPACE}" -f "${INFRA_DIR}/alloy-configmap.yaml"
          kubectl apply -n "${NAMESPACE}" -f "${INFRA_DIR}/aws-auth.yaml"
          kubectl apply -n "${NAMESPACE}" -f "${INFRA_DIR}/env-configmap.yaml"
      
          awk -v token="${BST_TOKEN}" '{gsub(/PLACEHOLDER_BST_TOKEN_REPLACE_AT_RUNTIME/, token)}1' "${INFRA_DIR}/deployment.yaml" > /tmp/deployment.yaml
      
          if grep -q "PLACEHOLDER_BST_TOKEN_REPLACE_AT_RUNTIME" /tmp/deployment.yaml; then
            echo "ERROR: Token injection failed - placeholder still exists"
            exit 1
          fi
      
          # Apply deployment and capture names
          kubectl apply -n "${NAMESPACE}" -f /tmp/deployment.yaml -o name | tee /tmp/applied_names.txt >/dev/null
          echo "Applied resources:"
          cat /tmp/applied_names.txt
      
          DEPLOYMENT_NAME="$(grep '^deployment\.apps/' /tmp/applied_names.txt | sed -n '1p' | sed 's#deployment\.apps/##')"
          if [ -z "${DEPLOYMENT_NAME}" ]; then
            echo "ERROR: Could not detect deployment name from kubectl output."
            exit 1
          fi
          echo "Detected deployment: ${DEPLOYMENT_NAME}"
      
          kubectl apply -n "${NAMESPACE}" -f "${INFRA_DIR}/service.yaml"
      
          kubectl rollout restart -n "${NAMESPACE}" deployment "${DEPLOYMENT_NAME}"
      
          echo "Waiting for rollout..."
          if ! kubectl rollout status -n "${NAMESPACE}" deployment "${DEPLOYMENT_NAME}" --timeout=300s; then
            echo ""
            echo "==== ROLLOUT FAILED: DEBUG INFO ===="
            echo ""
      
            echo ">> Deployment describe:"
            kubectl describe -n "${NAMESPACE}" deployment "${DEPLOYMENT_NAME}" || true
      
            echo ""
            echo ">> ReplicaSets for this deployment:"
            kubectl get -n "${NAMESPACE}" rs -l "app=${DEPLOYMENT_NAME%-depl}" -o wide || true
            # Fallback: show newest 5 ReplicaSets in namespace
            echo ""
            echo ">> Newest ReplicaSets (namespace):"
            kubectl get -n "${NAMESPACE}" rs --sort-by=.metadata.creationTimestamp | tail -n 5 || true
      
            # Identify newest RS for this deployment and describe it (THIS usually reveals FailedCreate reason)
            RS_NAME="$(kubectl get -n "${NAMESPACE}" rs -l "app=${DEPLOYMENT_NAME%-depl}" -o jsonpath='{.items[-1:].metadata.name}' 2>/dev/null || true)"
            if [ -n "${RS_NAME}" ]; then
              echo ""
              echo ">> Describe newest ReplicaSet: ${RS_NAME}"
              kubectl describe -n "${NAMESPACE}" rs "${RS_NAME}" || true
            else
              echo ""
              echo ">> Could not auto-detect RS by label; describing all RS matching release=prometheus:"
              kubectl describe -n "${NAMESPACE}" rs -l "release=prometheus" || true
            fi
      
            echo ""
            echo ">> Events (namespace, last 80):"
            kubectl get -n "${NAMESPACE}" events --sort-by=.metadata.creationTimestamp | tail -n 80 || true
      
            echo ""
            echo ">> Pods matching deployment selector (if any):"
            SELECTOR="$(kubectl describe -n "${NAMESPACE}" deployment "${DEPLOYMENT_NAME}" | awk -F': ' '/^Selector:/{print $2; exit}')"
            echo "Selector: ${SELECTOR}"
            if [ -n "${SELECTOR}" ]; then
              kubectl get -n "${NAMESPACE}" pods -l "${SELECTOR}" -o wide || true
            fi
      
            echo ""
            echo "Failing workflow because rollout timed out."
            exit 1
          fi
      
          echo "Rollout successful."
          kubectl get -n "${NAMESPACE}" pods -o wide
