name: CD - Deploy to Cloud

on:
  push:
    branches: [ master ]

# Prevent parallel deploys to the same branch
concurrency:
  group: cd-${{ github.ref }}
  cancel-in-progress: true

jobs:
  deploy:
    name: Build Docker image and Push to AWS ECR and Deploy to EKS
    runs-on: prod-eks-runners
    timeout-minutes: 40

    env:
      AWS_REGION: ap-south-1
      EKS_CLUSTER: backstract-dev
      ECR_REPO: backstract_apps
      INFRA_DIR: infra/k8s

      # Keep this only if you still need it somewhere else.
      # Rollout restart will NOT depend on this anymore.
      STATIC_TAG: nostalgic-beaver-26a2a5222a

    steps:
      - name: Check out code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup job (debug)
        timeout-minutes: 2
        run: |
          set -euo pipefail
          echo "Runner environment: $(uname -a)"
          echo "Disk:"
          df -h
          echo "Memory:"
          free -h
          echo "CPU:"
          nproc || true
          echo "Docker:"
          docker version || true
          docker info || true
          echo "Job setup completed"

      # Install AWS CLI v2 (GitHub-hosted runners have it; your ARC runner does not)
      - name: Install AWS CLI v2
        timeout-minutes: 5
        run: |
          set -euo pipefail

          if command -v aws >/dev/null 2>&1; then
            echo "AWS CLI already installed: $(aws --version)"
            exit 0
          fi

          if command -v apt-get >/dev/null 2>&1; then
            sudo apt-get update -y
            sudo apt-get install -y unzip curl
          elif command -v yum >/dev/null 2>&1; then
            sudo yum install -y unzip curl
          else
            echo "ERROR: Neither apt-get nor yum found. Cannot install AWS CLI automatically."
            exit 1
          fi

          curl -sSLo /tmp/awscliv2.zip "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip"
          unzip -q /tmp/awscliv2.zip -d /tmp
          sudo /tmp/aws/install --update

          aws --version

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.31.0'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Sanity check AWS identity
        run: |
          set -euo pipefail
          aws sts get-caller-identity

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Build & push using BuildKit cache stored in ECR
      - name: Build and push to ECR (BuildKit + registry cache)
        timeout-minutes: 25
        run: |
          set -euo pipefail
          date

          AWS_ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"
          REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          IMAGE="${REGISTRY}/${ECR_REPO}"
          SHA_TAG="${GITHUB_SHA}"

          echo "AWS_ACCOUNT_ID=${AWS_ACCOUNT_ID}"
          echo "REGISTRY=${REGISTRY}"
          echo "IMAGE=${IMAGE}"
          echo "SHA_TAG=${SHA_TAG}"

          if [ -z "${AWS_ACCOUNT_ID}" ] || [ -z "${REGISTRY}" ] || [ -z "${IMAGE}" ]; then
            echo "ERROR: Registry/Image variables are empty. Check AWS credentials/ECR access."
            exit 1
          fi

          docker buildx create --use --name builder >/dev/null 2>&1 || docker buildx use builder

          echo "Building and pushing:"
          echo "  ${IMAGE}:${SHA_TAG}"
          echo "  ${IMAGE}:${STATIC_TAG}"
          echo "Using cache ref:"
          echo "  ${IMAGE}:buildcache"

          time docker buildx build \
            --platform linux/amd64 \
            -t "${IMAGE}:${SHA_TAG}" \
            -t "${IMAGE}:${STATIC_TAG}" \
            --cache-from "type=registry,ref=${IMAGE}:buildcache" \
            --cache-to "type=registry,ref=${IMAGE}:buildcache,mode=max" \
            --push \
            .

          date

      - name: Update kubeconfig to point to EKS
        timeout-minutes: 8
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${EKS_CLUSTER}" --region "${AWS_REGION}"
          kubectl cluster-info

      # ✅ FIXED: rollout restart uses the actual deployment name returned by kubectl
      - name: Deploy to AWS EKS Cluster (apply + wait)
        timeout-minutes: 12
        env:
          BST_TOKEN: ${{ secrets.BACKSTRACT_AUTH_TOKEN }}
        run: |
          set -euo pipefail
      
          if [ -z "${BST_TOKEN:-}" ]; then
            echo "ERROR: BACKSTRACT_AUTH_TOKEN is empty or not set"
            exit 1
          fi
      
          kubectl apply -f "${INFRA_DIR}/alloy-rbac.yaml"
          kubectl apply -f "${INFRA_DIR}/alloy-configmap.yaml"
          kubectl apply -f "${INFRA_DIR}/aws-auth.yaml"
          kubectl apply -f "${INFRA_DIR}/env-configmap.yaml"
      
          # Inject token into deployment template
          awk -v token="${BST_TOKEN}" '{gsub(/PLACEHOLDER_BST_TOKEN_REPLACE_AT_RUNTIME/, token)}1' "${INFRA_DIR}/deployment.yaml" > /tmp/deployment.yaml
      
          if grep -q "PLACEHOLDER_BST_TOKEN_REPLACE_AT_RUNTIME" /tmp/deployment.yaml; then
            echo "ERROR: Token injection failed - placeholder still exists"
            exit 1
          fi
      
          # ✅ IMPORTANT: Avoid SIGPIPE by capturing apply output with tee (no head/awk early-exit)
          kubectl apply -f /tmp/deployment.yaml -o name | tee /tmp/applied_names.txt >/dev/null
      
          echo "Applied resources:"
          cat /tmp/applied_names.txt
      
          # Extract the first deployment name from the file (safe; kubectl already finished)
          DEPLOYMENT_NAME="$(grep '^deployment\.apps/' /tmp/applied_names.txt | sed -n '1p' | sed 's#deployment\.apps/##')"
      
          if [ -z "${DEPLOYMENT_NAME}" ]; then
            echo "ERROR: Could not detect deployment name from kubectl output."
            exit 1
          fi
      
          echo "Detected deployment: ${DEPLOYMENT_NAME}"
      
          kubectl apply -f "${INFRA_DIR}/service.yaml"
      
          kubectl rollout restart deployment "${DEPLOYMENT_NAME}"
          kubectl rollout status deployment "${DEPLOYMENT_NAME}" --timeout=300s
      
          kubectl get pods -o wide
